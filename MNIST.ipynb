{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification, Deep neural network\n",
    "### Handwritten digit recognition.\n",
    "\n",
    "The dataset provides 70k images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "\n",
    "with_info provides a tuple containing info about version, features, samples of the dataset\n",
    "\n",
    "as_supervised will load the dataset as a 2-tuple structure [input,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the tuple with the train and test sets\n",
    "\n",
    "so let's separate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'test': <PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_data['train'], mnist_data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist_info tells us we have a train set of 60k and a test set of 10k but we don't have a validation set so we need to make one ourselves, as the most popular choice we are going to use 10% of the rain set as the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       " 'test': <SplitInfo num_examples=10000, num_shards=1>}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info.splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dimensions of our sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "#here we don't know if the resulting number is an integer so let's convert it\n",
    "number_of_validation_samples = tf.cast(number_of_validation_samples, tf.int64)\n",
    "#let's also get the number of samples in the test set\n",
    "number_of_test_samples = mnist_info.splits['test'].num_examples\n",
    "number_of_test_samples = tf.cast(number_of_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to scale the data in some way to make the data more numerically stable, like values between 0 and 1 so we make a function that does that, it takes an mnist image and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_function(image, label):\n",
    "    #let's make sure the values are of type float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    #the mnist set contains values between 0 and 255 so let's divide this value by it\n",
    "    image /= 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale_function)\n",
    "test_data = mnist_test.map(scale_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin thinking about batches we need to make sure the data is not ordered so we need to shuffle it to not confuse the stochastic gradient descent algorithm.\n",
    "\n",
    "This dataset is small so it doesn't matter but it's good to set a buffer size that takes into account the system's limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can slice off our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = shuffled_train_and_validation_data.take(number_of_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we reduce the train data by that same amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = shuffled_train_and_validation_data.skip(number_of_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start thinking about batch size for the mini batch gradient descent method, the number should be 1 < batch size < number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we apply that number to the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model expects the validation set to be batched as well so batch the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.batch(number_of_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.batch(number_of_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iter is the pythonic syntax for making the validation set iterable, by default that will make the dataset iterable but will not load the data, next will load the next batch. Since there's only one batch it will load the inputs and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the set has 784 different images\n",
    "input_size = 784\n",
    "#and there are 10 different digits in total\n",
    "output_size = 10\n",
    "#we can refine the layers later\n",
    "hidden_layer_size = 256\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #we know the image dimensions already, 28x28, and flatten them into a vector\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    #the number of layers is up to us\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    #the output layer is created the same as the rest but we use the output size\n",
    "    #the activation function of the output layer must transform the value into a probability\n",
    "    #so we use softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam is a safe option for optimization\n",
    "\n",
    "as for the loss function i'm going to use sparse_categorical_crossentropy because i haven't applied one_hot_encoding to the data and it will do it for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=*.**)\n",
    "# if you want to change the learning rate of the optimizer\n",
    "# then just replace 'adam' with adam_optimizer in the compile module below\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. At the beginning of each epoch the training loss will be set to 0\n",
    "2. The algorithm will iterate over a preset number of batches\n",
    "3. The weights and biases will be updated after each batch\n",
    "4. At the end of each epoch will get a value for the loss function , indicating how the training is going\n",
    "5. We will also know the training accuracy\n",
    "6. At the end of each epoch the algorithm will forward propagate the whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "720/720 - 5s - loss: 0.0521 - accuracy: 0.9866 - val_loss: 0.0547 - val_accuracy: 0.9873 - 5s/epoch - 8ms/step\n",
      "Epoch 2/5\n",
      "720/720 - 4s - loss: 0.0326 - accuracy: 0.9904 - val_loss: 0.0469 - val_accuracy: 0.9877 - 4s/epoch - 6ms/step\n",
      "Epoch 3/5\n",
      "720/720 - 4s - loss: 0.0244 - accuracy: 0.9926 - val_loss: 0.0336 - val_accuracy: 0.9905 - 4s/epoch - 6ms/step\n",
      "Epoch 4/5\n",
      "720/720 - 5s - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.0377 - val_accuracy: 0.9930 - 5s/epoch - 7ms/step\n",
      "Epoch 5/5\n",
      "720/720 - 5s - loss: 0.0222 - accuracy: 0.9941 - val_loss: 0.0243 - val_accuracy: 0.9935 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2398f10d150>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_epochs = 5\n",
    "\n",
    "model.fit(train_data, epochs = number_of_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a bunch of times with different learning rates, layer sizes and such and i think that is decent for a practice project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 212ms/step - loss: 0.1430 - accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that after testing on a completely new dataset we got an accuracy of 98%, we can get better results by changing the batch size, learning rate, layer density, number of epochs, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
